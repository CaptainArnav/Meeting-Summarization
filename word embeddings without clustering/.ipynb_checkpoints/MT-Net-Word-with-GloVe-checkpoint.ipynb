{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7270526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75ba085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e79782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "meetings = np.load('../data/obj/meetings.npz')['arr_0'] # (num_meetings, num_turns, seq_len)\n",
    "summary = np.load('../data/obj/summary.npz')['arr_0'] # (num_meetings, summary_len)\n",
    "\n",
    "turns = np.load('../data/obj/turns.npz')['arr_0'] # (num_meetings, num_turns)\n",
    "role_vector = np.load('../data/obj/role_vector.npz')['arr_0'] # (num_roles, MAX_LENGTH_BIN = 3)\n",
    "\n",
    "with open('../data/obj/tokenizer.pickle', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "    \n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "embedding_dimension = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7364423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path) :\n",
    "    '''\n",
    "    parameters : file_path - path of file where embeddings are stored (eg: '<path>/glove.6B/glove.6B.100d.txt')\n",
    "    load the words and their respective embeddings from the GloVe file and set up a dictionary mapping \n",
    "    of words and their corresponding embeddings (embeddings will be stored in a numpy array of shape (d, ))\n",
    "    returns : embedding_dict - dictionary mapping of {word:embedding}\n",
    "    ''' \n",
    "    \n",
    "    embedding_dict = {}\n",
    "    file = open(file_path)\n",
    "    for line in file :\n",
    "        data = line.split(\" \")\n",
    "        word = data[0]\n",
    "        embedding = np.asarray(data[1:], dtype='float32')\n",
    "        embedding_dict[word] = embedding\n",
    "    file.close()\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd79ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embedding_matrix(embedding_dict, vocabulary, embedding_dimension) :\n",
    "    '''\n",
    "    parameters : embedding dict - dictionary mapping of {word:embedding} \n",
    "                 vocabulary - list of words in the training dataset\n",
    "                 embedding_dimension - dimension of word embeddings used in the model\n",
    "    initialises the embedding matrix with the ith row corresponding to the embedding of the ith word in the vocabulary\n",
    "    dimension of the embedding depends on the \n",
    "    returns : embedding_matrix of shape (n, d) where n is the number of words in the vocabulary and d is the \n",
    "              dimension of the embeddings\n",
    "    '''\n",
    "    \n",
    "    embedding_matrix = np.random.normal(0, 0.1, (len(vocabulary) + 1, embedding_dimension))\n",
    "    for i, word in enumerate(vocabulary) :\n",
    "        if word in embedding_dict.keys() :\n",
    "            word_embedding = embedding_dict[word]\n",
    "            embedding_matrix[i] = word_embedding\n",
    "    return embedding_matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71eb79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_embedding_layer(vocabulary_size, embedding_dimension) :\n",
    "    '''\n",
    "    parameters : vocabulary_size - size of vocabulary used\n",
    "                 embedding_dimension - integer which indicated the dimension of the word embeddings\n",
    "                 max_length - maximum length of the input to the model(eg : maximum length of an input sentence)\n",
    "    creates the embedding layer with trainable set to true so that weights cannot be changed during training.\n",
    "    Weights of the embedding layer follow normal distribution with mean=0, stddev=0.1\n",
    "    returns : embedding_layer \n",
    "    '''\n",
    "    \n",
    "    embedding_matrix = np.random.normal(0, 0.1, (vocabulary_size, embedding_dimension))\n",
    "    embedding_layer = layers.Embedding(input_dim=vocabulary_size,\n",
    "                                      output_dim=embedding_dimension,\n",
    "                                      weights=[embedding_matrix],\n",
    "                                      trainable=True,\n",
    "                                      name='embedding_rand')\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def get_static_embedding_layer(embedding_matrix) :\n",
    "    '''\n",
    "    parameters : embedding_matrix - numpy array of shape (n, d) used to set the weights of the embedding layer\n",
    "                 max_length - maximum length of the input to the model(eg : maximum length of an input sentence)\n",
    "    creates the embedding layer and sets its weights with trainable set to false \n",
    "    so that weights cannot be changed during training\n",
    "    returns : embedding_layer \n",
    "    '''\n",
    "    \n",
    "    embedding_layer = layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                                      output_dim=embedding_matrix.shape[1],\n",
    "                                      weights=[embedding_matrix],\n",
    "                                      trainable=False,\n",
    "                                      name='embedding_static')\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def get_dynamic_embedding_layer(embedding_matrix) :\n",
    "    '''\n",
    "    parameters : embedding_matrix - numpy array of shape (n, d) used to set the weights of the embedding layer\n",
    "                 max_length - maximum length of the input to the model(eg : maximum length of an input sentence)\n",
    "    creates the embedding layer and sets its weights with trainable set to true \n",
    "    so that weights can be changed or fine-tuned during training\n",
    "    returns : embedding_layer \n",
    "    '''\n",
    "    \n",
    "    embedding_layer = layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                                      output_dim=embedding_matrix.shape[1],\n",
    "                                      weights=[embedding_matrix],\n",
    "                                      trainable=True,\n",
    "                                      name='embedding_dynamic')\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e7cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer) :\n",
    "    def __init__(self) :\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        \n",
    "    def call(self, word_embeddings) :\n",
    "        '''\n",
    "        parameters : word_embeddings - tensor of shape (num_turns, seq_len, embed_dim)\n",
    "        returns : embeddings_with_position - tensor of shape (num_turns, seq_len, embed_dim)\n",
    "        '''\n",
    "        positional_embeddings = np.zeros((word_embeddings.shape[1], word_embeddings.shape[2]))\n",
    "        for i  in range(positional_embeddings.shape[0]) :\n",
    "            if i % 2 == 0 :\n",
    "                positional_embeddings[i] = np.array([np.sin(i/(1000 ** (2 * j / positional_embeddings.shape[1]))) for j in range(positional_embeddings.shape[1])])\n",
    "            else :\n",
    "                positional_embeddings[i] = np.array([np.cos(i/(1000 ** (2 * j / positional_embeddings.shape[1]))) for j in range(positional_embeddings.shape[1])])\n",
    "        \n",
    "        positional_embeddings = np.repeat(positional_embeddings[np.newaxis, :, :], word_embeddings.shape[0], axis=0)\n",
    "        \n",
    "        embeddings_with_position = positional_embeddings + word_embeddings\n",
    "        return embeddings_with_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ff4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(layers.Layer) :\n",
    "    def __init__(self, is_mask=False) :\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.is_mask = is_mask\n",
    "        \n",
    "    def call(self, query, key, value) :\n",
    "        '''\n",
    "        parameters : query - tensor of shape (num_turns, num_heads, seq_len_q, dim) \n",
    "                     key - tensor of shape (num_turns, num_heads, seq_len_k, dim) \n",
    "                     value - tensor of shape (num_turns, num_heads, seq_len_v, dim) \n",
    "                     **seq_len_k == seq_len_v\n",
    "        returns : attention - tensor of shape (num_turns, num_heads, seq_len, dim) \n",
    "        '''\n",
    "        # (num_turns, num_heads, seq_len_q, seq_len_k)\n",
    "        pre_attention = tf.linalg.matmul(query, key, transpose_b=True) / np.sqrt(key.shape[1])\n",
    "\n",
    "        if self.is_mask is True :\n",
    "            mask = np.zeros((pre_attention.shape[-2], pre_attention.shape[-1]))\n",
    "            mask.fill(-1e10)            \n",
    "            mask = np.triu(mask, k=1)\n",
    "            pre_attention = tf.math.multiply(pre_attention, mask)\n",
    "            \n",
    "        attention_weights = tf.nn.softmax(pre_attention, axis=-1)\n",
    "        \n",
    "        # (num_turns, num_heads, seq_len_q, dim)\n",
    "        attention = tf.linalg.matmul(attention_weights, value)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224cd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer) :\n",
    "    def __init__(self, embedding_dimension, num_heads, is_mask=False) :\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.dim = self.embedding_dimension // self.num_heads\n",
    "        self.is_mask = is_mask\n",
    "        \n",
    "        assert(self.dim * self.num_heads == self.embedding_dimension), \"embedding_dimension should be divisible by num_heads.\"\n",
    "\n",
    "        self.query_layer = layers.Dense(self.embedding_dimension)\n",
    "        self.key_layer = layers.Dense(self.embedding_dimension)\n",
    "        self.value_layer = layers.Dense(self.embedding_dimension)\n",
    "        \n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(is_mask=self.is_mask)\n",
    "        \n",
    "        self.linear_layer = layers.Dense(self.embedding_dimension)\n",
    "    \n",
    "    \n",
    "    def split_heads(self, input_tensor) :\n",
    "        '''\n",
    "        parameters : input_tensor - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        returns : input_tensor - resize tensor of shape (num_turns, num_heads, seq_len, dim)\n",
    "        '''\n",
    "        input_tensor = tf.reshape(input_tensor, (input_tensor.shape[0], -1, self.num_heads, self.dim))\n",
    "        return tf.transpose(input_tensor, [0,2,1,3])\n",
    "        \n",
    "        \n",
    "    def call(self, query, key, value) :\n",
    "        '''\n",
    "        parameters : query - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "                     key - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "                     value - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        returns : res - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        '''\n",
    "\n",
    "        query = self.query_layer(query)\n",
    "        key = self.key_layer(key)\n",
    "        value = self.value_layer(value)\n",
    "        \n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        \n",
    "        attention = self.scaled_dot_product_attention(query, key, value) # (num_turns, num_heads, seq_len, dim)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) # (num_turns, seq_len, num_heads, dim)\n",
    "        concat_attention = tf.reshape(attention, (attention.shape[0], -1, self.embedding_dimension)) # (num_turns, seq_len, embedding_dimension)\n",
    "        \n",
    "        res = self.linear_layer(concat_attention) # (num_turns, seq_len, embedding_dimension)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55f70467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddandNorm(layers.Layer) :\n",
    "    def __init__(self) :\n",
    "        super(AddandNorm, self).__init__()\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, input_tensor, skip_connection) :\n",
    "        '''\n",
    "        parameters : input_tensor - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "                     skip_connection - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        returns : res - normalized tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        '''\n",
    "        res = input_tensor + skip_connection\n",
    "        res = self.layer_norm(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3681aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(layers.Layer) :\n",
    "    def __init__(self, hidden_dim, output_dim) :\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layer_1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.layer_2 = layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, input_tensor) :\n",
    "        '''\n",
    "        parameters : input_tensor - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        returns : input_tensor - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        '''\n",
    "        res = self.layer_1(input_tensor)\n",
    "        res = self.layer_2(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b17581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_role_vector(x, role_vector, turn_seq) :\n",
    "    '''\n",
    "    parameters : x - tensor of shape (num_turns, seq_len, embed_dim)\n",
    "                 role_vector - tensor of shape (num_roles, role_vector_size)\n",
    "                 turn_seq - np array of shape (num_turns, ) representing the sequence of turns in a meeting\n",
    "    returns : concat_vector - concatenated vector of '<BOS>' tag and role_vector for each turn\n",
    "                              of shape (num_turns, 1, embed_dim + role_vector_size)\n",
    "    '''\n",
    "\n",
    "    turn = x[:, :1, :]\n",
    "\n",
    "    unpacked_turn_seq = tf.unstack(turn_seq)\n",
    "    \n",
    "    role = tf.expand_dims(tf.convert_to_tensor([role_vector[j] for j in unpacked_turn_seq]), 1)\n",
    "    turn_with_role = tf.concat([turn, role], axis=2) # (num_turns, 1, embed_dim + role_vector_size)\n",
    "    \n",
    "    return turn_with_role\n",
    "\n",
    "# x = tf.random.uniform((10, 100, 512))\n",
    "# role_vector = tf.random.uniform((2, 32))\n",
    "# turn_seq = [0,1,1,0,1,0,1,0,1,0]\n",
    "\n",
    "# print(concat_role_vector(x, role_vector, turn_seq).shape)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b50973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(layers.Layer) :\n",
    "    def __init__(self, embedding_dimension, num_heads) :\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dimension, num_heads)\n",
    "        self.add_and_norm_1 = AddandNorm()\n",
    "        self.feed_forward = FeedForward(200, embedding_dimension)\n",
    "        self.add_and_norm_2 = AddandNorm()\n",
    "        \n",
    "    def call(self, input_tensor) :\n",
    "        '''\n",
    "        parameters : input_tensor - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        returns : input_tensor - tensor of shape (num_turns, seq_len, embedding_dimension)\n",
    "        '''\n",
    "        res = self.multi_head_attention(input_tensor, input_tensor, input_tensor)\n",
    "        res_skip = self.add_and_norm_1(res, input_tensor)\n",
    "        res = self.feed_forward(res_skip)\n",
    "        res = self.add_and_norm_2(res, res_skip)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64bb7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(layers.Layer) :\n",
    "    def __init__(self, embedding_dimension, num_heads) :\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(embedding_dimension, num_heads, is_mask=True)\n",
    "        self.add_and_norm_1 = AddandNorm()\n",
    "        \n",
    "        self.multi_head_attention_1 = MultiHeadAttention(embedding_dimension, num_heads)\n",
    "        self.add_and_norm_2 = AddandNorm()\n",
    "        \n",
    "        self.multi_head_attention_2 = MultiHeadAttention(embedding_dimension, num_heads)\n",
    "        self.add_and_norm_3 = AddandNorm()\n",
    "        \n",
    "        self.feed_forward = FeedForward(200, embedding_dimension)\n",
    "        self.add_and_norm_4 = AddandNorm()\n",
    "        \n",
    "    def call(self, input_tensor, sentence_level_encoder_output, turn_level_encoder_output) :\n",
    "        res = self.masked_multi_head_attention(input_tensor, input_tensor, input_tensor)\n",
    "        res_skip = self.add_and_norm_1(res, input_tensor)\n",
    "        \n",
    "        res = self.multi_head_attention_1(res_skip, sentence_level_encoder_output, sentence_level_encoder_output)\n",
    "        res_skip = self.add_and_norm_2(res, res_skip)\n",
    "        \n",
    "        res = self.multi_head_attention_2(res_skip, turn_level_encoder_output, turn_level_encoder_output)\n",
    "        res_skip = self.add_and_norm_3(res, res_skip)\n",
    "        \n",
    "        res = self.feed_forward(res_skip)\n",
    "        res = self.add_and_norm_4(res, res_skip)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687cba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer) :\n",
    "    def __init__(self, \n",
    "                 num_blocks, \n",
    "                 embedding_dimension, \n",
    "                 num_heads) :\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        self.encoder_blocks = [EncoderBlock(embedding_dimension, num_heads) for _ in range(num_blocks)]\n",
    "        \n",
    "    def call(self, input_tensor) :\n",
    "        '''\n",
    "        parameters : input_tensor : tensor of shape (num_turns, seq_len, embed_dim) / (num_turns, 1, embed_dim + role_vector_size)\n",
    "        returns : x - tensor of shape (num_turns, seq_len, embed_dim) / (num_turns, 1, embed_dim + role_vector_size)\n",
    "        '''\n",
    "        x = input_tensor\n",
    "        \n",
    "        for i in range(self.num_blocks) :\n",
    "            x = self.encoder_blocks[i](x) # (num_turns, seq_len, embed_dim) \n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dabd9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 5, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_blocks=2, \n",
    "                         embedding_dimension=100, \n",
    "                         num_heads=10)\n",
    "\n",
    "input_tensor = tf.random.uniform((10, 5, 100), dtype=tf.float64, minval=0, maxval=200)\n",
    "\n",
    "output_tensor = sample_encoder(input_tensor)\n",
    "\n",
    "output_tensor.shape  # (num_turns, seq_len, embed_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02496faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer) :\n",
    "    def __init__(self, \n",
    "                 num_blocks, \n",
    "                 embedding_dimension, \n",
    "                 num_heads) :\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        self.decoder_blocks = [DecoderBlock(embedding_dimension, num_heads) for _ in range(num_blocks)]\n",
    "        \n",
    "    def call(self, input_tensor, word_level_encoder_output, turn_level_encoder_output) :\n",
    "        '''\n",
    "        parameters : input_tensor : tensor of shape (batch_size=1, target_seq_len, embed_dim)\n",
    "        returns : x - tensor of shape (batch_size=1, target_seq_len, embed_dim) \n",
    "        '''\n",
    "        x = input_tensor\n",
    "        for i in range(self.num_blocks) :\n",
    "            x = self.decoder_blocks[i](x, word_level_encoder_output, turn_level_encoder_output)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2bc7b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 7, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_blocks=2, \n",
    "                         embedding_dimension=100, \n",
    "                         num_heads=10)\n",
    "\n",
    "input_tensor = tf.random.uniform((1, 7, 100), dtype=tf.float64, minval=0, maxval=200)\n",
    "word_level_tensor = tf.random.uniform((10, 5, 100), dtype=tf.float64, minval=0, maxval=200)\n",
    "turn_level_tensor = tf.random.uniform((10, 1, 132), dtype=tf.float64, minval=0, maxval=200)\n",
    "\n",
    "output_tensor = sample_decoder(input_tensor, word_level_tensor, turn_level_tensor)\n",
    "\n",
    "output_tensor.shape  # (batch_size=1, target_seq_len, embed_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53af8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTNet(tf.keras.Model) :\n",
    "    def __init__(self, \n",
    "                 num_blocks, \n",
    "                 embedding_dimension, \n",
    "                 num_heads_word, \n",
    "                 num_heads_turn, \n",
    "                 num_heads_dec, \n",
    "                 vocabulary_size,\n",
    "                 embedding_matrix,\n",
    "                 role_vector_size, \n",
    "                 init_role_vector, \n",
    "                 mode='static') :\n",
    "        \n",
    "        super(MTNet, self).__init__()\n",
    "        \n",
    "        self.init_role_vector = init_role_vector\n",
    "                \n",
    "        self.embedding_layer = None\n",
    "        if mode == 'static' :\n",
    "            self.embedding_layer = get_static_embedding_layer(embedding_matrix)\n",
    "        elif mode == 'dynamic' :\n",
    "            self.embedding_layer = get_dynamic_embedding_layer(embedding_matrix)\n",
    "        elif mode == 'rand' :\n",
    "            self.embedding_layer = get_rand_embedding_layer(vocabulary_size, embedding_dimension) \n",
    "            \n",
    "        self.positional_embedding_layer = PositionalEmbedding()\n",
    "                                                            \n",
    "        \n",
    "        self.word_level_encoder = Encoder(num_blocks, \n",
    "                                          embedding_dimension, \n",
    "                                          num_heads_word)\n",
    "        \n",
    "        self.turn_level_encoder = Encoder(num_blocks, \n",
    "                                          embedding_dimension + role_vector_size, \n",
    "                                          num_heads_turn)\n",
    "        \n",
    "        self.role_vector = layers.Dense(role_vector_size)\n",
    "        \n",
    "        self.decoder = Decoder(num_blocks, \n",
    "                               embedding_dimension, \n",
    "                               num_heads_dec)\n",
    "        \n",
    "        self.fully_connected_layer = layers.Dense(vocabulary_size)\n",
    "        \n",
    "    def call(self, input_tensor, target_tensor, turn_seq) :\n",
    "        \n",
    "        input_tensor = tf.squeeze(input_tensor, [0]) # (num_turns, seq_len)\n",
    "        \n",
    "        embedding_input = self.embedding_layer(input_tensor) # (num_turns, seq_len, embed_dim)\n",
    "        embedding_input = self.positional_embedding_layer(embedding_input) # (num_turns, seq_len, embed_dim)\n",
    "        \n",
    "        x1 = self.word_level_encoder(embedding_input) # (num_turns, seq_len, embed_dim)\n",
    "\n",
    "        input_role_vector = self.role_vector(self.init_role_vector) # (num_roles, role_vector_size)\n",
    "\n",
    "        x1_concat = concat_role_vector(x1, input_role_vector, turn_seq) # (num_turns, 1, embed_dim + role_vector_size)\n",
    "\n",
    "        x2 = self.turn_level_encoder(x1_concat)\n",
    "        \n",
    "        x1 = tf.reshape(x1, [1, -1, x1.shape[-1]]) # (1, num_turns * seq_len, embed_dim)\n",
    "        x2 = tf.reshape(x2, [1, -1, x2.shape[-1]]) # (1, num_turns * seq_len, embed_dim)\n",
    "        \n",
    "        \n",
    "        target_x = self.embedding_layer(target_tensor) # (num_turns, seq_len, embed_dim)\n",
    "        target_x = self.positional_embedding_layer(target_x) # (num_turns, seq_len, embed_dim)\n",
    "        \n",
    "        x = self.decoder(target_x, x1, x2) #(batch_size=1, target_seq_len, embed_dim)\n",
    "        x = self.fully_connected_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "050436bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 1068])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_role_vector = tf.random.uniform((2, 2))\n",
    "turn_seq = np.array([0,1,1,0,1,0,1,0,1,0])\n",
    "temp_embedding_matrix = np.random.normal(0, 0.1, (vocabulary_size, embedding_dimension))\n",
    "\n",
    "sample_mtnet = MTNet(num_blocks=2, \n",
    "                     embedding_dimension=100, \n",
    "                     num_heads_word=10, \n",
    "                     num_heads_turn=11,\n",
    "                     num_heads_dec=10, \n",
    "                     vocabulary_size=vocabulary_size,\n",
    "                     role_vector_size=32, \n",
    "                     init_role_vector=trial_role_vector, \n",
    "                     embedding_matrix=temp_embedding_matrix,\n",
    "                     mode='static')\n",
    "\n",
    "temp_input = tf.random.uniform((1, 10, 5), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((1, 3), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out = sample_mtnet(temp_input, temp_target, turn_seq)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9c5c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((meetings, summary))\n",
    "dataset = dataset.batch(1)\n",
    "\n",
    "# meetings1, summary1= next(iter(dataset))\n",
    "# meetings1.shape, summary1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a804102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Glove embeddings(100 dimensional) and convert it into a dictionary with mapping {word:embedding}\n",
    "\n",
    "file_path = '../GloVe/glove.6B/glove.6B.100d.txt'\n",
    "embedding_dict = load_embeddings(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99f2ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tokenizer.word_index.keys()\n",
    "embedding_matrix = set_embedding_matrix(embedding_dict, vocabulary, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dba00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.1, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6dd7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1f448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7de07fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtnet = MTNet(num_blocks=2, \n",
    "             embedding_dimension=100, \n",
    "             num_heads_word=5, \n",
    "             num_heads_turn=5,\n",
    "             num_heads_dec=5, \n",
    "             vocabulary_size=vocabulary_size,  \n",
    "             embedding_matrix=embedding_matrix,\n",
    "             role_vector_size=10, \n",
    "             init_role_vector=role_vector,\n",
    "             mode='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a35d7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(1, 70, 100), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(1, 100), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(70), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(input_tensor, target_tensor, turn_seq):\n",
    "\n",
    "    target_inp = target_tensor[:, :-1]\n",
    "    target_real = target_tensor[:, 1:]\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = mtnet(input_tensor, target_inp, turn_seq)\n",
    "        loss = loss_function(target_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, mtnet.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mtnet.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(target_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e15bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "# input_tensor = np.array(tf.random.uniform((1, 10, 9), dtype=tf.int64, minval=0, maxval=200))\n",
    "# target_tensor = np.array(tf.random.uniform((1, 6), dtype=tf.int64, minval=0, maxval=200))\n",
    "# input_role_vector = np.array(tf.random.uniform((2, 2)))\n",
    "# turn_seq = [0,1,1,0,1,0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9500e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x7f8f1553b940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function train_step at 0x7f8f1553b940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1 Batch 0 Loss 7.1638 Accuracy 0.0101\n",
      "Epoch 1 Batch 1 Loss 6.8698 Accuracy 0.0657\n",
      "Epoch 1 Batch 2 Loss 8.1748 Accuracy 0.0842\n",
      "Epoch 1 Batch 3 Loss 10.1034 Accuracy 0.0707\n",
      "Epoch 1 Loss 10.1034 Accuracy 0.0707\n",
      "Time taken for 1 epoch: 2.78 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 14.0738 Accuracy 0.0606\n",
      "Epoch 2 Batch 1 Loss 18.7467 Accuracy 0.0303\n",
      "Epoch 2 Batch 2 Loss 18.6552 Accuracy 0.0202\n",
      "Epoch 2 Batch 3 Loss 18.1591 Accuracy 0.0202\n",
      "Epoch 2 Loss 18.1591 Accuracy 0.0202\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 5.6584 Accuracy 0.0303\n",
      "Epoch 3 Batch 1 Loss 5.6425 Accuracy 0.0758\n",
      "Epoch 3 Batch 2 Loss 7.2884 Accuracy 0.0505\n",
      "Epoch 3 Batch 3 Loss 7.0646 Accuracy 0.0429\n",
      "Epoch 3 Loss 7.0646 Accuracy 0.0429\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 7.3371 Accuracy 0.0000\n",
      "Epoch 4 Batch 1 Loss 6.0785 Accuracy 0.0606\n",
      "Epoch 4 Batch 2 Loss 6.0555 Accuracy 0.0808\n",
      "Epoch 4 Batch 3 Loss 6.1766 Accuracy 0.0707\n",
      "Epoch 4 Loss 6.1766 Accuracy 0.0707\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 5.3622 Accuracy 0.1111\n",
      "Epoch 5 Batch 1 Loss 5.1692 Accuracy 0.0707\n",
      "Epoch 5 Batch 2 Loss 5.0712 Accuracy 0.0505\n",
      "Epoch 5 Batch 3 Loss 5.0647 Accuracy 0.0455\n",
      "Epoch 5 Loss 5.0647 Accuracy 0.0455\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 4.8603 Accuracy 0.0303\n",
      "Epoch 6 Batch 1 Loss 5.0401 Accuracy 0.0455\n",
      "Epoch 6 Batch 2 Loss 4.9796 Accuracy 0.0640\n",
      "Epoch 6 Batch 3 Loss 5.0036 Accuracy 0.0682\n",
      "Epoch 6 Loss 5.0036 Accuracy 0.0682\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 4.6507 Accuracy 0.0303\n",
      "Epoch 7 Batch 1 Loss 4.9233 Accuracy 0.0202\n",
      "Epoch 7 Batch 2 Loss 4.7725 Accuracy 0.0471\n",
      "Epoch 7 Batch 3 Loss 4.7358 Accuracy 0.0530\n",
      "Epoch 7 Loss 4.7358 Accuracy 0.0530\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 4.8432 Accuracy 0.0303\n",
      "Epoch 8 Batch 1 Loss 4.6941 Accuracy 0.0758\n",
      "Epoch 8 Batch 2 Loss 4.5905 Accuracy 0.0909\n",
      "Epoch 8 Batch 3 Loss 4.6552 Accuracy 0.0783\n",
      "Epoch 8 Loss 4.6552 Accuracy 0.0783\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 4.9567 Accuracy 0.1111\n",
      "Epoch 9 Batch 1 Loss 4.8938 Accuracy 0.1162\n",
      "Epoch 9 Batch 2 Loss 4.6618 Accuracy 0.1178\n",
      "Epoch 9 Batch 3 Loss 4.6547 Accuracy 0.0985\n",
      "Epoch 9 Loss 4.6547 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 4.7140 Accuracy 0.1111\n",
      "Epoch 10 Batch 1 Loss 4.6480 Accuracy 0.1162\n",
      "Epoch 10 Batch 2 Loss 4.5235 Accuracy 0.1178\n",
      "Epoch 10 Batch 3 Loss 4.6118 Accuracy 0.0985\n",
      "Epoch 10 Loss 4.6118 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 4.6693 Accuracy 0.1111\n",
      "Epoch 11 Batch 1 Loss 4.7075 Accuracy 0.1162\n",
      "Epoch 11 Batch 2 Loss 4.5454 Accuracy 0.1178\n",
      "Epoch 11 Batch 3 Loss 4.5895 Accuracy 0.0985\n",
      "Epoch 11 Loss 4.5895 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 4.5505 Accuracy 0.1111\n",
      "Epoch 12 Batch 1 Loss 4.5765 Accuracy 0.1162\n",
      "Epoch 12 Batch 2 Loss 4.4766 Accuracy 0.1111\n",
      "Epoch 12 Batch 3 Loss 4.5483 Accuracy 0.1035\n",
      "Epoch 12 Loss 4.5483 Accuracy 0.1035\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 4.5359 Accuracy 0.0606\n",
      "Epoch 13 Batch 1 Loss 4.6745 Accuracy 0.0354\n",
      "Epoch 13 Batch 2 Loss 4.5122 Accuracy 0.0640\n",
      "Epoch 13 Batch 3 Loss 4.5288 Accuracy 0.0581\n",
      "Epoch 13 Loss 4.5288 Accuracy 0.0581\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 4.6144 Accuracy 0.1111\n",
      "Epoch 14 Batch 1 Loss 4.6146 Accuracy 0.1162\n",
      "Epoch 14 Batch 2 Loss 4.4747 Accuracy 0.1178\n",
      "Epoch 14 Batch 3 Loss 4.5145 Accuracy 0.0985\n",
      "Epoch 14 Loss 4.5145 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4.6642 Accuracy 0.1111\n",
      "Epoch 15 Batch 1 Loss 4.6931 Accuracy 0.0606\n",
      "Epoch 15 Batch 2 Loss 4.5226 Accuracy 0.0808\n",
      "Epoch 15 Batch 3 Loss 4.5438 Accuracy 0.0707\n",
      "Epoch 15 Loss 4.5438 Accuracy 0.0707\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 4.5818 Accuracy 0.1111\n",
      "Epoch 16 Batch 1 Loss 4.5868 Accuracy 0.1162\n",
      "Epoch 16 Batch 2 Loss 4.4612 Accuracy 0.1178\n",
      "Epoch 16 Batch 3 Loss 4.5078 Accuracy 0.0985\n",
      "Epoch 16 Loss 4.5078 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.17 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 4.6081 Accuracy 0.1111\n",
      "Epoch 17 Batch 1 Loss 4.6341 Accuracy 0.1162\n",
      "Epoch 17 Batch 2 Loss 4.4826 Accuracy 0.1178\n",
      "Epoch 17 Batch 3 Loss 4.5167 Accuracy 0.0985\n",
      "Epoch 17 Loss 4.5167 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 4.5785 Accuracy 0.1111\n",
      "Epoch 18 Batch 1 Loss 4.5991 Accuracy 0.1162\n",
      "Epoch 18 Batch 2 Loss 4.4614 Accuracy 0.1178\n",
      "Epoch 18 Batch 3 Loss 4.4997 Accuracy 0.0985\n",
      "Epoch 18 Loss 4.4997 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 4.5705 Accuracy 0.1111\n",
      "Epoch 19 Batch 1 Loss 4.6227 Accuracy 0.1162\n",
      "Epoch 19 Batch 2 Loss 4.4766 Accuracy 0.1178\n",
      "Epoch 19 Batch 3 Loss 4.5134 Accuracy 0.0985\n",
      "Epoch 19 Loss 4.5134 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 4.5555 Accuracy 0.1111\n",
      "Epoch 20 Batch 1 Loss 4.5769 Accuracy 0.1162\n",
      "Epoch 20 Batch 2 Loss 4.4493 Accuracy 0.1178\n",
      "Epoch 20 Batch 3 Loss 4.4883 Accuracy 0.0985\n",
      "Epoch 20 Loss 4.4883 Accuracy 0.0985\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (meetings1, summary1)) in enumerate(dataset):\n",
    "\n",
    "        train_step(meetings1, summary1, turns[batch])\n",
    "\n",
    "#         if batch % 5 == 0:\n",
    "        print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#         print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05313cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
