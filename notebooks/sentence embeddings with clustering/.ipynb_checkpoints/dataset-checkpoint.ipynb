{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab36e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from numpy import savez_compressed\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7217a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_text(sentence, is_summary) :\n",
    "    '''\n",
    "    parameters : sentence - string\n",
    "    Adds ' ' before punctuations, replace numbers or special characters with ' '.\n",
    "    adds the '<BOS>' (Beginning of Sentence) tag to each sentence\n",
    "    returns : de-noised/pre-processed sentence\n",
    "    '''\n",
    "    \n",
    "    sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", sentence)\n",
    "    \n",
    "    if is_summary :\n",
    "        sentence = '[BEGIN] ' + sentence\n",
    "    else :\n",
    "        sentence = '[BOS] ' + sentence\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8901c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./split_0.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "'''\n",
    "results is a list of dictionaries with keys -> 'id', 'meeting', 'summary'. Meeting consists of 'speaker' and 'utt'\n",
    "turns is a 2d list consisting of speakers in each meeting -> 0, 1, 2 etc\n",
    "meetings is a 2d list consisting of corresponding turns' sentences \n",
    "summaries is a 2d list consisting of corresponding meeting's summary\n",
    "'''\n",
    "results = []\n",
    "turns = []\n",
    "meetings = []\n",
    "summaries = []\n",
    "\n",
    "NUM_TURNS = 70\n",
    "\n",
    "for json_str in json_list:\n",
    "    results.append(json.loads(json_str))\n",
    "\n",
    "for result in results :\n",
    "    turns_temp = []\n",
    "    meetings_temp = []\n",
    "    for obj in result['meeting'][:NUM_TURNS] :\n",
    "        turns_temp.append(ord(obj['speaker']) - 65) # convert letter to number i.e 'A' -> 0, 'B' -> 1\n",
    "#         turns_temp.append(obj['role'])\n",
    "        sentence = ' '.join(obj['utt']['word'])\n",
    "        meetings_temp.append(preprocess_text(sentence, is_summary=False))\n",
    "\n",
    "    turns.append(turns_temp)\n",
    "    meetings.append(meetings_temp)\n",
    "    \n",
    "    summary = ' '.join(result['summary'])\n",
    "    summaries.append(preprocess_text(summary, is_summary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ff7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "for meeting in meetings :\n",
    "    all_text += meeting\n",
    "all_text += summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d20e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenizer(sentences) :\n",
    "    '''\n",
    "    parameters : sentences - list of sentences\n",
    "    creates a vocabulary of words based on the list of inputted sentences using the Tokenizer object\n",
    "    returns : tokenizer - Tokenizer object\n",
    "    '''\n",
    "    \n",
    "    tokenizer = Tokenizer(filters='', oov_token='[UNKNOWN]')\n",
    "#     tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_texts(sentences, tokenizer) :\n",
    "    '''\n",
    "    parameters : sentences - list of sentences\n",
    "                 tokenizer - Tokenizer object initialized using dataset\n",
    "    encodes the text sequences in the dataset by mapping the index of the word in the vocabulary to each word\n",
    "    in the dataset\n",
    "    returns : encoded_docs - list of encoded sentences\n",
    "    '''\n",
    "    \n",
    "    encoded_docs = tokenizer.texts_to_sequences(sentences)\n",
    "    return encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c7c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = make_tokenizer(all_text)\n",
    "\n",
    "padded_meetings = []\n",
    "for meeting in meetings :\n",
    "    padded_meetings.append(pad_sequences(encode_texts(meeting, tokenizer), maxlen=100, padding='post'))\n",
    "    \n",
    "padded_turns = pad_sequences(turns, padding='post')\n",
    "\n",
    "padded_summaries = pad_sequences(encode_texts(summaries, tokenizer), maxlen=100, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391d15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH_BIN = 3\n",
    "role_vector = []\n",
    "\n",
    "max_turn_number = np.max(turns)\n",
    "\n",
    "for num in range(max_turn_number + 1) :\n",
    "    binary_list = list(bin(num).replace('0b', '').zfill(MAX_LENGTH_BIN))\n",
    "    role_vector.append(np.array(list(map(int, binary_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799183b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_meetings, num_turns, seq_len)\n",
    "savez_compressed('meetings.npz', np.int64(np.array(padded_meetings)))\n",
    "# (num_meetings, num_turns)\n",
    "savez_compressed('turns.npz', np.int64(np.array(padded_turns)))\n",
    "# (num_meetings, summary_len)\n",
    "savez_compressed('summary.npz', np.int64(np.array(padded_summaries)))\n",
    "# (num_roles, 3)\n",
    "savez_compressed('role_vector.npz', np.int64(np.array(role_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb40d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a5010a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "meetings = np.load('meetings.npz')['arr_0'] # (num_meetings, num_turns, seq_len)\n",
    "turns = np.load('turns.npz')['arr_0'] # (num_meetings, num_turns)\n",
    "summary = np.load('summary.npz')['arr_0'] # (num_meetings, summary_len)\n",
    "role_vector = np.load('role_vector.npz')['arr_0'] # (num_meetings, num_turns, 3)\n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8181e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((meetings, summary, turns))\n",
    "dataset = dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a473853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 70, 100]),\n",
       " TensorShape([1, 100]),\n",
       " TensorShape([1, 70]),\n",
       " (4, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meetings1, summary1, turns1 = next(iter(dataset))\n",
    "meetings1.shape, summary1.shape, turns1.shape, role_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b86dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
